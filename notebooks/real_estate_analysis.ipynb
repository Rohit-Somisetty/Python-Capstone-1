{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cec441",
   "metadata": {},
   "source": [
    "# Real Estate Data Processing Pipeline\n",
    "\n",
    "This notebook demonstrates the implementation and usage of the RealEstateDataset class for processing real estate data.\n",
    "\n",
    "## Objectives:\n",
    "1. Implement the RealEstateDataset class\n",
    "2. Load and explore the housing dataset\n",
    "3. Clean and preprocess the data\n",
    "4. Generate insights and visualizations\n",
    "5. Save the cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778de99",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, let's install the necessary Python packages for our real estate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8404f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        'pandas>=1.5.0',\n",
    "        'numpy>=1.21.0',\n",
    "        'matplotlib>=3.5.0',\n",
    "        'seaborn>=0.11.0'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            print(f\"âœ… Successfully installed {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Failed to install {package}: {e}\")\n",
    "\n",
    "# Uncomment the line below to install packages\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3272a6d",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad505004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(f\"ğŸ¼ Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f439216",
   "metadata": {},
   "source": [
    "## 3. RealEstateDataset Class Implementation\n",
    "\n",
    "Here we implement the RealEstateDataset class with the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealEstateDataset:\n",
    "    \"\"\"\n",
    "    A class for handling real estate data processing including loading,\n",
    "    cleaning, and analyzing real estate datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the RealEstateDataset instance.\"\"\"\n",
    "        self.data: Optional[pd.DataFrame] = None\n",
    "        self.original_data: Optional[pd.DataFrame] = None\n",
    "        self.filepath: Optional[str] = None\n",
    "    \n",
    "    def load_data(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read the dataset and initialize a pandas DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file containing real estate data\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.filepath = filepath\n",
    "            self.data = pd.read_csv(filepath)\n",
    "            # Keep a copy of original data for comparison\n",
    "            self.original_data = self.data.copy()\n",
    "            \n",
    "            print(f\"âœ… Data loaded successfully from {filepath}\")\n",
    "            print(f\"ğŸ“Š Dataset shape: {self.data.shape}\")\n",
    "            print(\"\\nğŸ” First 5 rows:\")\n",
    "            display(self.data.head())\n",
    "            print(\"\\nğŸ“‹ Column names:\")\n",
    "            print(self.data.columns.tolist())\n",
    "            print(\"\\nğŸ·ï¸  Data types:\")\n",
    "            print(self.data.dtypes)\n",
    "            \n",
    "            return self.data\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ Error: File '{filepath}' not found.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def clean_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing and invalid data.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned dataset\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"âŒ No data loaded. Please call load_data() first.\")\n",
    "        \n",
    "        print(\"ğŸ§¹ Starting data cleaning process...\")\n",
    "        \n",
    "        # Store initial state\n",
    "        initial_rows = len(self.data)\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        print(\"\\nğŸ“Š Missing values before cleaning:\")\n",
    "        missing_before = self.data.isnull().sum()\n",
    "        print(missing_before[missing_before > 0])\n",
    "        \n",
    "        # Numerical columns - fill with median\n",
    "        numerical_cols = self.data.select_dtypes(include=[np.number]).columns\n",
    "        for col in numerical_cols:\n",
    "            if self.data[col].isnull().sum() > 0:\n",
    "                median_val = self.data[col].median()\n",
    "                self.data[col] = self.data[col].fillna(median_val)\n",
    "                print(f\"   âœ“ Filled {col} missing values with median: {median_val:.2f}\")\n",
    "        \n",
    "        # Categorical columns - fill with mode or drop\n",
    "        categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if self.data[col].isnull().sum() > 0:\n",
    "                if self.data[col].isnull().sum() / len(self.data) > 0.5:\n",
    "                    # If more than 50% missing, drop the column\n",
    "                    self.data = self.data.drop(columns=[col])\n",
    "                    print(f\"   âœ“ Dropped column '{col}' (>50% missing)\")\n",
    "                else:\n",
    "                    # Fill with mode\n",
    "                    mode_val = self.data[col].mode()[0] if not self.data[col].mode().empty else 'Unknown'\n",
    "                    self.data[col] = self.data[col].fillna(mode_val)\n",
    "                    print(f\"   âœ“ Filled {col} missing values with mode: '{mode_val}'\")\n",
    "        \n",
    "        # 2. Handle invalid entries\n",
    "        print(\"\\nğŸ” Checking for invalid entries...\")\n",
    "        \n",
    "        # Remove negative prices\n",
    "        if 'Price' in self.data.columns:\n",
    "            negative_prices = self.data['Price'] < 0\n",
    "            if negative_prices.sum() > 0:\n",
    "                self.data = self.data[~negative_prices]\n",
    "                print(f\"   âœ“ Removed {negative_prices.sum()} rows with negative prices\")\n",
    "        \n",
    "        # Remove invalid room counts\n",
    "        for col in ['Bedrooms', 'Bathrooms']:\n",
    "            if col in self.data.columns:\n",
    "                invalid_rooms = self.data[col] < 0\n",
    "                if invalid_rooms.sum() > 0:\n",
    "                    self.data = self.data[~invalid_rooms]\n",
    "                    print(f\"   âœ“ Removed {invalid_rooms.sum()} rows with negative {col}\")\n",
    "        \n",
    "        # Remove invalid sizes\n",
    "        if 'Size_sqft' in self.data.columns:\n",
    "            invalid_size = self.data['Size_sqft'] <= 0\n",
    "            if invalid_size.sum() > 0:\n",
    "                self.data = self.data[~invalid_size]\n",
    "                print(f\"   âœ“ Removed {invalid_size.sum()} rows with invalid size\")\n",
    "        \n",
    "        # 3. Fix data types\n",
    "        print(\"\\nğŸ”§ Fixing data types...\")\n",
    "        \n",
    "        # Convert date columns\n",
    "        date_columns = ['Date_Added']\n",
    "        for col in date_columns:\n",
    "            if col in self.data.columns:\n",
    "                try:\n",
    "                    self.data[col] = pd.to_datetime(self.data[col], errors='coerce')\n",
    "                    print(f\"   âœ“ Converted {col} to datetime\")\n",
    "                except:\n",
    "                    print(f\"   âš ï¸  Could not convert {col} to datetime\")\n",
    "        \n",
    "        # 4. Remove duplicates\n",
    "        duplicates = self.data.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            self.data = self.data.drop_duplicates()\n",
    "            print(f\"   âœ“ Removed {duplicates} duplicate rows\")\n",
    "        \n",
    "        # 5. Reset index\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        \n",
    "        # Summary\n",
    "        final_rows = len(self.data)\n",
    "        rows_removed = initial_rows - final_rows\n",
    "        \n",
    "        print(f\"\\nâœ… Data cleaning completed!\")\n",
    "        print(f\"ğŸ“Š Initial rows: {initial_rows}\")\n",
    "        print(f\"ğŸ“Š Final rows: {final_rows}\")\n",
    "        print(f\"ğŸ“Š Rows removed: {rows_removed}\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def describe_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Print basic statistics and exploratory insights.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary containing various statistics and insights\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"âŒ No data loaded. Please call load_data() first.\")\n",
    "        \n",
    "        print(\"ğŸ“ˆ REAL ESTATE DATA ANALYSIS REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Basic info\n",
    "        print(f\"\\nğŸ“Š DATASET OVERVIEW\")\n",
    "        print(f\"   â€¢ Total properties: {len(self.data):,}\")\n",
    "        print(f\"   â€¢ Number of features: {len(self.data.columns)}\")\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        numerical_cols = self.data.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) > 0:\n",
    "            print(f\"\\nğŸ“Š NUMERICAL STATISTICS\")\n",
    "            display(self.data[numerical_cols].describe())\n",
    "        \n",
    "        insights = {}\n",
    "        \n",
    "        # Property type analysis\n",
    "        if 'Type' in self.data.columns:\n",
    "            print(f\"\\nğŸ  PROPERTY TYPE DISTRIBUTION\")\n",
    "            type_counts = self.data['Type'].value_counts()\n",
    "            type_percentages = self.data['Type'].value_counts(normalize=True) * 100\n",
    "            \n",
    "            for prop_type, count in type_counts.items():\n",
    "                percentage = type_percentages[prop_type]\n",
    "                print(f\"   â€¢ {prop_type}: {count:,} ({percentage:.1f}%)\")\n",
    "            \n",
    "            insights['property_types'] = type_counts.to_dict()\n",
    "        \n",
    "        # Price analysis by property type\n",
    "        if 'Price' in self.data.columns and 'Type' in self.data.columns:\n",
    "            print(f\"\\nğŸ’° AVERAGE PRICES BY PROPERTY TYPE\")\n",
    "            avg_prices = self.data.groupby('Type')['Price'].agg(['mean', 'median', 'count'])\n",
    "            display(avg_prices)\n",
    "            \n",
    "            insights['avg_prices_by_type'] = avg_prices['mean'].to_dict()\n",
    "        \n",
    "        # Size analysis by location\n",
    "        if 'Size_sqft' in self.data.columns and 'Location' in self.data.columns:\n",
    "            print(f\"\\nğŸ“ AVERAGE SIZE BY LOCATION (Top 10)\")\n",
    "            avg_size = self.data.groupby('Location')['Size_sqft'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "            display(avg_size.head(10))\n",
    "            \n",
    "            insights['avg_size_by_location'] = avg_size['mean'].head(10).to_dict()\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def save_cleaned_data(self, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the cleaned dataset to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path where to save the cleaned data\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"âŒ No data to save. Please load and clean data first.\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            self.data.to_csv(output_path, index=False)\n",
    "            print(f\"âœ… Cleaned data saved to: {output_path}\")\n",
    "            print(f\"ğŸ“Š Saved {len(self.data)} rows and {len(self.data.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "print(\"ğŸ—ï¸  RealEstateDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8942b4",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Initial Exploration\n",
    "\n",
    "Now let's create an instance of our RealEstateDataset class and load the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RealEstateDataset\n",
    "dataset = RealEstateDataset()\n",
    "\n",
    "# Load the housing data\n",
    "data_path = '../data/raw/housing_data.csv'\n",
    "dataset.load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04f14a",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning\n",
    "\n",
    "Clean the dataset by handling missing values and invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97653333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "cleaned_data = dataset.clean_data()\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"\\nğŸ“Š Missing values after cleaning:\")\n",
    "missing_after = dataset.data.isnull().sum()\n",
    "remaining_missing = missing_after[missing_after > 0]\n",
    "if len(remaining_missing) == 0:\n",
    "    print(\"   âœ… No missing values remaining!\")\n",
    "else:\n",
    "    print(remaining_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e58bd",
   "metadata": {},
   "source": [
    "## 6. Data Analysis and Insights\n",
    "\n",
    "Generate descriptive statistics and insights from the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada82ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "insights = dataset.describe_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797af6d",
   "metadata": {},
   "source": [
    "## 7. Data Visualizations\n",
    "\n",
    "Create visualizations to better understand the real estate market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "def create_real_estate_visualizations(data):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for real estate data analysis.\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Real Estate Market Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Price distribution\n",
    "    if 'Price' in data.columns:\n",
    "        axes[0, 0].hist(data['Price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Property Price Distribution')\n",
    "        axes[0, 0].set_xlabel('Price ($)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].ticklabel_format(style='plain', axis='x')\n",
    "    \n",
    "    # 2. Property type distribution\n",
    "    if 'Type' in data.columns:\n",
    "        type_counts = data['Type'].value_counts()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(type_counts)))\n",
    "        axes[0, 1].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "        axes[0, 1].set_title('Property Type Distribution')\n",
    "    \n",
    "    # 3. Price vs Size scatter plot\n",
    "    if 'Price' in data.columns and 'Size_sqft' in data.columns:\n",
    "        axes[1, 0].scatter(data['Size_sqft'], data['Price'], alpha=0.6, color='purple')\n",
    "        axes[1, 0].set_title('Price vs Property Size')\n",
    "        axes[1, 0].set_xlabel('Size (sqft)')\n",
    "        axes[1, 0].set_ylabel('Price ($)')\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        correlation = data['Price'].corr(data['Size_sqft'])\n",
    "        axes[1, 0].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                       transform=axes[1, 0].transAxes, fontsize=10,\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 4. Average price by location (top 10)\n",
    "    if 'Price' in data.columns and 'Location' in data.columns:\n",
    "        avg_price_by_location = data.groupby('Location')['Price'].mean().sort_values(ascending=True).tail(10)\n",
    "        axes[1, 1].barh(range(len(avg_price_by_location)), avg_price_by_location.values, color='orange')\n",
    "        axes[1, 1].set_title('Average Price by Location (Top 10)')\n",
    "        axes[1, 1].set_xlabel('Average Price ($)')\n",
    "        axes[1, 1].set_ylabel('Location')\n",
    "        axes[1, 1].set_yticks(range(len(avg_price_by_location)))\n",
    "        axes[1, 1].set_yticklabels(avg_price_by_location.index)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ“Š Visualizations created successfully!\")\n",
    "\n",
    "# Create visualizations\n",
    "create_real_estate_visualizations(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bf22f",
   "metadata": {},
   "source": [
    "## 8. Additional Analysis\n",
    "\n",
    "Let's perform some additional analysis to gain deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis\n",
    "print(\"ğŸ” ADDITIONAL INSIGHTS\\n\")\n",
    "\n",
    "# Correlation matrix for numerical features\n",
    "numerical_cols = dataset.data.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_cols) > 1:\n",
    "    print(\"ğŸ“Š Correlation Matrix:\")\n",
    "    correlation_matrix = dataset.data[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Price analysis by bedrooms\n",
    "if 'Price' in dataset.data.columns and 'Bedrooms' in dataset.data.columns:\n",
    "    print(\"\\nğŸ›ï¸  Average Price by Number of Bedrooms:\")\n",
    "    price_by_bedrooms = dataset.data.groupby('Bedrooms')['Price'].agg(['mean', 'count']).sort_index()\n",
    "    display(price_by_bedrooms)\n",
    "\n",
    "# Market trends (if date information is available)\n",
    "if 'Date_Added' in dataset.data.columns:\n",
    "    dataset.data['Year_Added'] = dataset.data['Date_Added'].dt.year\n",
    "    if not dataset.data['Year_Added'].isnull().all():\n",
    "        print(\"\\nğŸ“… Properties Listed by Year:\")\n",
    "        yearly_counts = dataset.data['Year_Added'].value_counts().sort_index()\n",
    "        display(yearly_counts)\n",
    "\n",
    "print(\"\\nâœ… Additional analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd205e",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "output_path = '../data/cleaned/housing_data_cleaned.csv'\n",
    "dataset.save_cleaned_data(output_path)\n",
    "\n",
    "print(\"\\nğŸ‰ Real Estate Data Processing Pipeline completed successfully!\")\n",
    "print(\"\\nğŸ“‹ Summary:\")\n",
    "print(f\"   â€¢ Original dataset: {len(dataset.original_data)} rows\")\n",
    "print(f\"   â€¢ Cleaned dataset: {len(dataset.data)} rows\")\n",
    "print(f\"   â€¢ Data quality improved: {((len(dataset.data)/len(dataset.original_data)) * 100):.1f}% data retained\")\n",
    "print(f\"   â€¢ Cleaned data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe699b87",
   "metadata": {},
   "source": [
    "## 10. Test Environment Setup\n",
    "\n",
    "Let's verify that our environment is properly configured and all components are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155655dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment setup\n",
    "def test_environment():\n",
    "    \"\"\"\n",
    "    Test that all required packages are installed and working correctly.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Testing Environment Setup\\n\")\n",
    "    \n",
    "    # Test package imports\n",
    "    packages_to_test = {\n",
    "        'pandas': pd,\n",
    "        'numpy': np,\n",
    "        'matplotlib': plt,\n",
    "        'seaborn': sns\n",
    "    }\n",
    "    \n",
    "    for package_name, package in packages_to_test.items():\n",
    "        try:\n",
    "            version = getattr(package, '__version__', 'Unknown')\n",
    "            print(f\"   âœ… {package_name}: {version}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {package_name}: Error - {e}\")\n",
    "    \n",
    "    # Test RealEstateDataset class\n",
    "    try:\n",
    "        test_dataset = RealEstateDataset()\n",
    "        print(f\"   âœ… RealEstateDataset class: Working\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ RealEstateDataset class: Error - {e}\")\n",
    "    \n",
    "    # Test file paths\n",
    "    files_to_check = [\n",
    "        '../data/raw/housing_data.csv',\n",
    "        '../src/real_estate_dataset.py',\n",
    "        '../requirements.txt'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ“ File Structure Check:\")\n",
    "    for file_path in files_to_check:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"   âœ… {file_path}: Found\")\n",
    "        else:\n",
    "            print(f\"   âŒ {file_path}: Not found\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ Environment test completed!\")\n",
    "\n",
    "# Run environment test\n",
    "test_environment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
